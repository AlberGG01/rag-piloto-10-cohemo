# üéì Lecci√≥n Magistral: Ingenier√≠a de Sistemas RAG (Defense Contracts)

> **Profesor:** Principal Software Architect  
> **Tema:** Retrieval-Augmented Generation en Entornos Cr√≠ticos  
> **Nivel:** Avanzado / Ingenier√≠a de Precisi√≥n

---

Bienvenido al "quir√≥fano". Hoy no vamos a ver diagramas de cajas bonitas. Vamos a abrir el c√≥digo y entender la **f√≠sica** detr√°s de nuestro sistema de IA. Olvida la magia; esto son matem√°ticas y algoritmos.

---

## üèõÔ∏è 0. La Base Oculta: Normalizaci√≥n Sem√°ntica (The Structural Layer)

*Has preguntado: "¬øPor qu√© el sistema busca delimitadores `‚îÄ‚îÄ‚îÄ`? ¬øDe d√≥nde salen?"*
Excelente pregunta. Aqu√≠ es donde ganamos o perdemos la batalla de la calidad.

### El Problema del PDF Crudo
Un PDF no es texto; es una sopa de letras posicionada en coordenadas X/Y.
*   Si lees un contrato con `pdfplumber` o `PyPDF2`, obtienes:
    *   Cabeceras de p√°gina repetidas en medio de frases: *"Cl√°usula 5... [COHEMO 2024 P√°g 5] ...de rescisi√≥n."*
    *   Tablas rotas que se leen l√≠nea a l√≠nea en lugar de celda a celda.
    *   Notas al pie que rompen el flujo l√≥gico.

Si pasas esta "basura" al Chunker, tendr√°s chunks rotos.

### La Soluci√≥n: Fase 0 - Normalizaci√≥n con LLM (`src/utils/normalizer.py`)
Antes de que el RAG siquiera "vea" el documento, pasamos el texto crudo por un **LLM Normalizador (GPT-4o)**.

*   **Objetivo:** No es resumir. Es **Re-estructurar**.
*   **El Prompt M√°gico:** Le ordenamos al modelo actuar como un "Experto en Estructuraci√≥n".
    *   *"Detecta tablas y p√°salas a Markdown."*
    *   *"Elimina n√∫meros de p√°gina."*
    *   *Y lo m√°s importante:* **"Inserta el delimitador `‚îÄ‚îÄ‚îÄ` antes de cada secci√≥n l√≥gica."**

> **Resultado:** El RAG no ingesta el PDF original. Ingesta una versi√≥n "Plat√≥nica" ideal del contrato, limpia y etiquetada. Por eso el Chunker (`chunking.py`) puede buscar `‚îÄ‚îÄ‚îÄ` con confianza total. No adivina d√≥nde acaba la cl√°usula; el Normalizador ya se lo ha dicho expl√≠citamente.

---

## üèõÔ∏è 1. Estrategia de Fragmentaci√≥n (The Chunking Dilemma)

El primer error del novato es pensar que el texto es solo texto. Para una m√°quina, el texto es informaci√≥n espacial. C√≥mo cortamos ese texto define la "resoluci√≥n" de nuestra base de datos.

### üî¨ An√°lisis del Splitter (`src/utils/chunking.py`)
No usamos un `CharacterTextSplitter` ingenuo. Usamos una **Estrategia Sem√°ntica Jer√°rquica**.

*   **Naive Splitting (Lo que NO hacemos):** Cortar cada 500 caracteres ciegamente.
    *   *Riesgo:* Cortar√≠as una frase legal vital: *"El contratista pagar√°... [CORTE] ...una bonificaci√≥n"*. El cambio de sentido es catastr√≥fico.
*   **Nuestra Implementaci√≥n (`subdivide_large_section`):**
    1.  **Detecci√≥n de Fronteras:** Aprovechamos el trabajo de la Fase 0. Buscamos los `‚îÄ‚îÄ‚îÄ` que insert√≥ el Normalizador.
    2.  **Recursividad:** Si una secci√≥n mide >1000 tokens, no la cortamos arbitrariamente. Buscamos puntos y seguido.

### üìê Justificaci√≥n Matem√°tica: La Teor√≠a de la Diluci√≥n Vectorial

¬øPor qu√© 1000 tokens y no 8000 (el l√≠mite del modelo)?

Imagina que un Embeddings es el **promedio sem√°ntico** de todas las palabras en el chunk.
$$ \vec{V}_{chunk} \approx \frac{1}{N} \sum_{i=1}^{N} \vec{v}_{palabra_i} $$

*   **Chunk Peque√±o (100 tokens):** El vector es muy "puntiagudo". Apunta a un concepto muy espec√≠fico (ej. "Penalizaci√≥n por mora"). **Alta Precisi√≥n, Bajo Contexto.**
*   **Chunk Gigante (8000 tokens):** Tienes el contrato entero. El vector es el promedio de "Penalizaci√≥n", "Objeto", "Precios", "Firmas".
    *   *Efecto:* El vector resultante se queda en el "centro" del espacio sem√°ntico. Se vuelve **gris**. No se parece a nada espec√≠fico.
    *   *Consecuencia:* Cuando busques "Penalizaci√≥n", tu query (vector espec√≠fico) estar√° lejos del vector del chunk gigante (vector promedio). **El sistema fallar√° en encontrarlo.**

> **Veredicto:** Elegimos 1000 tokens como el "punto dulce" donde hay suficiente contexto legal para entender la cl√°usula, pero suficiente especificidad para que el vector siga apuntando a un tema concreto.

---

## üîé 2. Arquitectura de Recuperaci√≥n (The Retriever Matrix)

No todos los Retrievers son iguales. Aqu√≠ justificamos nuestra elecci√≥n.

### üìä La Matriz de Comparaci√≥n

| Tipo de Retriever | Descripci√≥n | ¬øPor qu√© lo usamos/descartamos? |
| :--- | :--- | :--- |
| **Naive Retriever** | Query $\to$ Vector Search $\to$ Top K Chunks. | **Inuficiente.** Si un contrato repite la palabra "Veh√≠culo" 200 veces, los Top-5 chunks vendr√°n *todos* del mismo documento. El usuario pierde la visi√≥n global. |
| **Parent Document Retriever** | Busca chunks peque√±os, pero devuelve al LLM el "Documento Padre" entero. | **Descartado.** Los contratos de defensa son PDFs de 100 p√°ginas. Inyectar el PDF entero satura la ventana de contexto de GPT-4o y dispara el coste. |
| **Hierarchical / Knowledge Graph** | Busca documentos primero, luego hace "Zoom-In" en sus partes. | **‚úÖ ELEGIDO (`smart_retrieval.py`).** Nos permite diversidad (encontrar 5 contratos distintos) y precisi√≥n (encontrar la cl√°usula exacta dentro de ellos). |

### üß† Teor√≠a Did√°ctica: El Desacople Index-Generation

En nuestra implementaci√≥n (`smart_retrieval.py`), aplicamos un desacople cr√≠tico:
1.  **Indexaci√≥n (Lo que buscamos):** Vectores densos de chunks enriquecidos.
2.  **Generaci√≥n (Lo que le damos al LLM):** No solo el chunk. Inyectamos metadata (`num_contrato`, `importe`).

El vector en la base de datos es solo una "huella digital". Cuando la encontramos, no le damos al LLM la huella; le damos el objeto completo con sus etiquetas. Esto permite que el LLM diga: *"Seg√∫n el Documento X..."* aunque esa etiqueta "Documento X" no fuera semanticamente relevante para la b√∫squeda vectorial per se.

### üî¨ Deep Dive: El Problema de la Diversidad (Naive vs Hierarchical)

*Preguntaste: "¬øPor qu√© busca documentos primero? ¬øQu√© gano?"*

Imagina esta Query: **"¬øQu√© penalizaciones por retraso existen en mis contratos?"**

Sup√≥n que tienes 2 contratos:
*   **Contrato A (Gigante):** Menciona la palabra "penalizaci√≥n" 50 veces (en cada cl√°usula).
*   **Contrato B (Peque√±o):** Menciona "penalizaci√≥n" solo 1 vez (en la cl√°usula clave).

#### Escenario 1: Naive Retrieval (B√∫squeda Simple)
Pides los Top-10 chunks m√°s similares.
*   **Resultado:** 10 chunks del **Contrato A**. (Porque al repetirlo tanto, estad√≠sticamente inunda el Top).
*   **Efecto:** El LLM te responde: *"Solo veo penalizaciones en el Contrato A"*. **Has perdido el Contrato B.** El sistema es "ciego" a la diversidad.

#### Escenario 2: Hierarchical Retrieval (Nuestra Soluci√≥n)
1.  **Fase de Exploraci√≥n:** Pedimos los Top-50 chunks. (Aqu√≠ salen 45 de A y 5 de B).
2.  **Agrupaci√≥n:**
    *   Doc A: Score promedio 0.95 (Muy relevante).
    *   Doc B: Score promedio 0.92 (Relevante).
3.  **Selecci√≥n de Diversidad:** Elegimos los **Top Documentos √önicos**: [A, B].
4.  **Zoom-In (Snippet Selection):**
    *   De Doc A: Dame tus 3 mejores chunks.
    *   De Doc B: Dame tus 3 mejores chunks.
*   **Resultado Final al LLM:** 3 chunks de A + 3 chunks de B.
*   **Respuesta del Bot:** *"Se han encontrado penalizaciones en el Contrato A y TAMBI√âN en el Contrato B..."* -> **√âXITO.**

---

## ü§ñ 3. La Orquestaci√≥n Ag√©ntica (El Cerebro Recursivo)

Aqu√≠ abandonamos los scripts lineales. Entramos en **Teor√≠a de Control**.

### üîÑ Linealidad vs. Recursividad
*   **Flujo Lineal:** Input $\to$ Search $\to$ Generate.
    *   *Fallo:* El usuario pregunta algo ambiguo. La b√∫squeda falla. El sistema devuelve "No lo s√©". Fin.
*   **Flujo Recursivo (Nuestro Sistema):**
    *   Input $\to$ Search $\to$ **Evaluate** $\to$ (¬øEs suficiente?) $\to$ **NO** $\to$ **Corrective** $\to$ Search Again.
    *   *Analog√≠a:* Es como un becario que no encuentra un archivo. En vez de rendirse (Lineal), vuelve a ti y te pregunta: *"¬øPodr√≠a estar archivado con otro nombre?"* (Recursivo).

### üö¶ Toma de Decisiones (Routing Logic)
Analiza `src/agents/evaluator.py`.
La funci√≥n de routing no es un `if keyword in text`. Es un **Juez LLM**.
*   Le damos al LLM los chunks recuperados y la pregunta.
*   Prompt: *"Teniendo estos datos, ¬øpuedes responder rigurosamente? Responde SUFFICIENT o INSUFFICIENT."*
*   Esta decisi√≥n es probabil√≠stica, no determinista. Es lo que hace al sistema "inteligente".

### üíæ Estado del Grafo (`src/graph/state.py`)
El `WorkflowState` es la memoria de corto plazo.
*   **Problema:** En recursividad, ¬øc√≥mo evitamos bucles infinitos?
*   **Soluci√≥n:** Campo `retry_count`.
*   El grafo monitorea cu√°ntas veces ha pasado por el nodo `corrective`. Si `retry_count > 3`, el sistema fuerza una salida (un "Circuit Breaker"), evitando gastar dinero infinito en un bucle sin soluci√≥n.

---

## üß† 4. El Post-Procesado (Re-ranking y Refinamiento)

### El "Segundo Filtro": Cross-Encoders
La base vectorial es "tonta". Usa **Similitud del Coseno**, que es geom√©trica.
*   *Query:* "Penalizaci√≥n por retraso"
*   *Chunk A:* "El retraso no conlleva penalizaci√≥n" (Negaci√≥n).
*   *Chunk B:* "Se aplicar√° penalizaci√≥n por retraso" (Afirmaci√≥n).

Para el vector, A y B son casi id√©nticos (comparten palabras clave). Tienen distancias muy cercanas.
Aqu√≠ entra el **Reranker (`src/utils/reranker.py`)**.
*   Act√∫a como un **Cross-Encoder**: Lee la Query y el Chunk A *juntos* y se pregunta: *"¬øResponde esto realmente a la pregunta?"*
*   El LLM ver√≠a que el Chunk A niega la premisa y le bajar√≠a el score, mientras que el Coseno lo puso arriba.
*   **Resultado:** Filtramos ruido sem√°ntico que matem√°ticamente parec√≠a correcto pero l√≥gicamente no lo era.

---

## üß™ 5. Gu√≠a de Supervivencia para el Junior

Si vas a tocar el c√≥digo, lee esto antes de hacer `git commit`.

### üìç Traceability (D√≥nde ocurre la magia)
*   **Si quieres cambiar el corte de texto:** `src/utils/chunking.py`, l√≠nea ~305 (`subdivide_large_section`) y `src/utils/normalizer.py` (el prompt).
*   **Si el bot alucina datos:** `src/agents/rag_agent.py`, busca `validate_response`. Aqu√≠ est√° el "Red Team" que frena las invenciones.
*   **Si la b√∫squeda falla:** `src/utils/smart_retrieval.py`, l√≠nea ~47. Revisa si `metadata_filters` est√° siendo demasiado agresivo y filtrando el documento correcto.

### üî¨ Laboratorio: Experimento Sugerido
¬øQuieres entender el "Signal Dilution" en vivo?
1.  Ve a `src/config.py`.
2.  Cambia `CHUNK_MAX_TOKENS` de `1000` a `4000`.
3.  Borra la DB (`rm -rf data/vectorstore`) y regenera (`python init_vectorstore.py`).
4.  Pregunta por un dato muy espec√≠fico (ej. "importe del aval del expediente 2024").
5.  **Observaci√≥n:** Ver√°s que el Retrieval falla o trae documentos irrelevantes. ¬øPor qu√©? Porque el "vector promedio" del chunk de 4000 tokens se ha diluido tanto que ya no apunta a "avales", apunta a "contrato gen√©rico".

---
## üìä 6. M√©tricas del Sistema Actual (Snapshot)

Estos son los "Constantes F√≠sicas" de nuestro universo RAG a d√≠a de hoy:

*   **Volumen de Datos:**
    *   **Contratos Ingestados:** 20 documentos (PDFs originales en `data/contracts`).
    *   **Documentos Normalizados:** 20 documentos (Markdown estructurado en `data/normalized`).
*   **Configuraci√≥n de Fragmentaci√≥n (`src/config.py`):**
    *   **Max Tokens:** 1000 tokens (El "punto dulce" de granulidad).
    *   **Overlap:** 100 tokens (Para preservar contexto entre cortes).
*   **Motor Vetorial:**
    *   **Modelo de Embeddings:** `text-embedding-3-large` (3072 dimensiones).
    *   **Store:** ChromaDB (Persistencia local en `data/vectorstore`).

---
*Fin de la lecci√≥n. Ahora, ve y compila.*
