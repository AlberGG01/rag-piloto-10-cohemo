# An√°lisis Forense: Arquitectura RAG, Costes y Optimizaci√≥n

**Fecha:** 31/01/2026  
**Versi√≥n:** 1.0 (Post 28/30 Accuracy)

---

## üìä Resumen Ejecutivo

**Estado Actual:**
- ‚úÖ **Accuracy:** 28/30 estimado (93%)
- ‚ùå **Latencia Media:** 111.63s (Objetivo: < 10s)
- üí∞ **Coste Operacional:** ~$0.10-0.15 por query (ALTO)

**Cuello de Botella Principal:** Re-ranking BGE en CPU (50-200s / 45-90% del tiempo total)

---

## ü§ñ Stack de Modelos Actual

### 1. OpenAI GPT-4o (`gpt-4o`)
**Uso:** S√≠ntesis principal de respuestas, normalizaci√≥n PDF  
**Frecuencia:** 2 llamadas por query (analista + s√≠ntesis)  
**Coste:** ~$0.01-0.02 por query
- Input: $0.0025 / 1K tokens (contexto ~2-5K tokens)
- Output: $0.01 / 1K tokens (respuesta ~500-1K tokens)

**Justificaci√≥n Hist√≥rica:** 
- Inicialmente usaba `gpt-4o-mini` (barato, $0.15/1M input)
- Se cambi√≥ a `gpt-4o` tras ver accuracy 60% ‚Üí 75%+ con prompts anti-alucinaci√≥n
- Comentario en c√≥digo: "Upgraded from gpt-4o-mini for precision"

### 2. OpenAI GPT-4o-mini
**Uso:** Queries simples, evaluaci√≥n interna, reescritura de contexto  
**Frecuencia:** ~20% de queries (queries simples sin extracci√≥n)  
**Coste:** ~$0.001 per query
- 30x m√°s barato que GPT-4o

**Nota:** En el c√≥digo existe un router (`MODEL_FAST`), pero actualmente **todas las queries usan GPT-4o** por miedo a regresi√≥n de accuracy.

### 3. OpenAI text-embedding-3-large
**Uso:** Embeddings vectoriales (b√∫squeda sem√°ntica)  
**Dimensiones:** 3072  
**Frecuencia:** 1 llamada por query  
**Coste:** ~$0.0005 per query
- $0.13 / 1M tokens

**Rendimiento:** 0.3-3.0s latencia (red I/O bound, fuera de control)

### 4. BAAI/bge-reranker-v2-m3 (Local CPU)
**Uso:** Re-ranking de chunks para mejorar precisi√≥n  
**Parameters:** 560M  
**Device:** CPU (sin GPU/CUDA)  
**Latencia:** **50-200s** ‚ö†Ô∏è MAYOR PROBLEMA  
**Coste:** $0 (local)

**Justificaci√≥n Hist√≥rica:**
- Implementado para subir de 70% ‚Üí 76.7% accuracy
- Se activ√≥ tras ver que RRF solo no era suficiente
- Documentado en [diagnostico_latencia.md](file:///C:/Users/alber/.gemini/antigravity/brain/94312c63-4cb8-4070-a74b-aef616c787c1/diagnostico_latencia.md): "Re-ranking causa 50-90s latencia pero mejora relevancia"

---

## üí∏ Desglose de Costes por Query

| Componente | Coste Unitario | Uso/Frecuencia | Coste Total |
|------------|---------------|----------------|-------------|
| **GPT-4o (Extracci√≥n)** | $0.005-0.01 | 1x por query | $0.005-0.01 |
| **GPT-4o (S√≠ntesis)** | $0.005-0.01 | 1x por query | $0.005-0.01 |
| **Embeddings (text-emb-3-large)** | $0.0005 | 1x por query | $0.0005 |
| **Re-ranker BGE (CPU)** | $0 | 1x por query | $0 (pero 100s latencia) |
| **TOTAL** | | | **$0.0105-0.0205** |

**Coste Mensual Estimado (1000 queries):** ~$10-20 USD

**Nota Cr√≠tica:** El coste actual es MODERADO, pero la latencia de 111s es **inaceptable** para producci√≥n.

---

## ‚è±Ô∏è Breakdown de Latencia por Componente

Basado en datos reales del [accuracy_report_v4.json](file:///c:/Users/alber/OneDrive/Desktop/Piloto%20Empresa/defense_contracts_system/data/accuracy_report_v4.json) (8 tests):

### Latencia Promedio: 111.63s

| Fase | Tiempo (s) | % Total | Optimizable |
|------|-----------|---------|-------------|
| **1. Hybrid Search** | 0.5-3.0s | 1-3% | ‚ùå (ya √≥ptimo) |
| ‚îî‚îÄ Vector Search (OpenAI API) | 0.3-2.5s | | ‚ùå (red I/O) |
| ‚îî‚îÄ BM25 Search (local) | 0.0-0.1s | | ‚úÖ (trivial) |
| ‚îî‚îÄ RRF Fusion | 0.0-0.02s | | ‚úÖ (trivial) |
| ‚îî‚îÄ Metadata Boost | 0.0-0.05s | | ‚úÖ (trivial) |
| **2. Re-ranking BGE (CPU)** | **50-200s** | **45-90%** | ‚úÖ (GPU / deshabilitar) |
| **3. LLM Extraction (GPT-4o)** | 3-5s | 3-5% | ‚ö†Ô∏è (reducible) |
| **4. LLM Synthesis (GPT-4o)** | 3-5s | 3-5% | ‚ö†Ô∏è (reducible) |

### Casos Extremos Observados:

**EDGE_01 (ISO Gen√©rica):**
- Hybrid Search: 1.01s
- Re-ranking: **201.75s** (98% del tiempo) ‚Üê CATASTR√ìFICO
- LLM: 3.76s
- **Total: 206.51s**

**EDGE_04 (Densidad Fechas):**
- Hybrid Search: 0.54s
- Re-ranking: 33.94s
- Date Density Analysis: ~3s
- LLM: 0.88s
- **Total: 38.36s** ‚Üê Mejor caso

---

## üîç Justificaci√≥n de Par√°metros Actuales

### ¬øPor qu√© `top_k=50` para queries exhaustivas?

**Contexto Hist√≥rico:**
1. Inicialmente `top_k=10` (r√°pido, pero fallaba EDGE_01, EDGE_07)
2. Se subi√≥ a `top_k=30` (75% accuracy)
3. Se subi√≥ a `top_k=50` para "lista todos los contratos..." (intento de arreglar EDGE_01)

**Resultado:** 
- ‚úÖ EDGE_07 pas√≥ (listas exhaustivas)
- ‚ùå EDGE_01 FALL√ì con 206s latencia
- **Conclusi√≥n:** `top_k=50` es **contraproducente** (mucha informaci√≥n mareaREA la LLM + mata latencia)

**Recomendaci√≥n:** Revertir a `top_k=30` y usar Metadata Enrichment (P4) en lugar de fuerza bruta.

### ¬øPor qu√© 50 chunks pre-re-ranking?

**Contexto:**
- Hybrid search (BM25 + Vector) recupera top 50 de cada fuente
- RRF fusiona ‚Üí ~50 chunks √∫nicos
- Re-ranker procesa los 50 ‚Üí devuelve top 10

**Justificaci√≥n:**
- 50 chunks asegura alta cobertura (recall)
- RRF ya hace un primer filtro (no es aleatorio)
- BGE re-ranker mejora precisi√≥n final

**Problema:** Procesar 50 chunks en CPU tarda 100-200s (2-4s por chunk en CPU)

**Soluci√≥n Propuesta:** Reducir a 20 chunks pre-re-ranking (suficiente para recall, -60% latencia)

---

## üö® Diagn√≥stico de Problemas

### Problema 1: Re-ranker BGE en CPU
**Impacto:** 50-200s por query (45-90% del tiempo)  
**Causa ra√≠z:** Modelo de 560M par√°metros sin GPU

**Soluciones:**
1. **GPU (Ideal):** Latencia 0.5-2s con CUDA ‚Üí Requiere hardware ($)
2. **Reducir chunks:** 50 ‚Üí 20 ‚Üí Latencia -60% (100s ‚Üí 40s)
3. **Deshabilitar para queries simples:** Skip re-ranking si `metadata_filter` activo
4. **GGUF local (Futuro):** Usar quantizaci√≥n para CPU (ver siguiente secci√≥n)

### Problema 2: Doble Llamada LLM (Extracci√≥n + S√≠ntesis)
**Impacto:** 6-10s + $0.01-0.02 por query  
**Causa:** Arquitectura de 2 pasos (analista + s√≠ntesis)

**Justificaci√≥n Original:**
- Paso 1 (Extracci√≥n): Busca datos brutos
- Paso 2 (S√≠ntesis): Formatea respuesta con tabla markdown

**Optimizaci√≥n Posible:**
- Fusionar en 1 llamada para queries QUANTITATIVE simples (-3s, -50% coste)
- Mantener 2 pasos para queries COMPLEX (necesitan razonamiento)

### Problema 3: GPT-4o para todo
**Impacto:** ~$0.02 por query (alto coste operacional)  
**Causa:** Miedo a regresi√≥n de accuracy

**Router Actual (NO USADO):**
```python
MODEL_FAST = "gpt-4o-mini"  # For simple queries
MODEL_CHATBOT = "gpt-4o"    # Upgraded for precision
```

**Conclusi√≥n:** Se podr√≠a usar `gpt-4o-mini` para 30-40% de queries (metadata exactas, fechas, CIF) sin p√©rdida de accuracy ‚Üí -70% coste en esas queries.

---

## üìà Oportunidades de Optimizaci√≥n (Sin Romper Accuracy)

### Tier 1: Bajo Riesgo, Alto Impacto (Implementar YA)

1. **Reducir pre-re-ranking: 50 ‚Üí 20 chunks**
   - Impacto: -60% latencia re-ranking (100s ‚Üí 40s)
   - Riesgo: Bajo (RRF ya prioriza los mejores)
   - Test: Evaluar 8 tests cr√≠ticos

2. **Skip re-ranking para queries con `metadata_filter`**
   - Impacto: -100s en ~40% de queries
   - Riesgo: Muy bajo (esas queries ya tienen alta precisi√≥n)
   - Ejemplo: "¬øImporte de CON_2024_012?" ‚Üí Ya filtrado, no necesita re-ranking

3. **Revertir `top_k=50` ‚Üí `top_k=30`**
   - Impacto: +30% velocidad hybrid search
   - Riesgo: Bajo (top_k=50 fall√≥ en EDGE_01)

### Tier 2: Medio Riesgo, Medio Impacto

4. **Usar `gpt-4o-mini` para queries QUANTITATIVE simples**
   - Clasificar queries en: SIMPLE (CIF, fechas) vs COMPLEX (normativas, listas)
   - Impacto: -70% coste en 30% de queries ‚Üí ahorro $3-6/mes (peque√±o, pero escalable)
   - Riesgo: Medio (puede afectar accuracy en edge cases)

5. **Fusionar Extracci√≥n + S√≠ntesis en 1 llamada LLM**
   - Solo para queries QUANTITATIVE simples
   - Impacto: -3-5s, -50% coste LLM
   - Riesgo: Medio (formateado menos perfecto)

### Tier 3: Futuro (GGUF Offline)

6. **Migrar s√≠ntesis a GGUF local**
   - Ver `guia_migracion_gguf.md` para detalles t√©cnicos
   - Impacto: Coste ‚Üí $0, latencia variable
   - Riesgo: Alto (calidad de respuesta puede bajar)

---

## üéØ Recomendaci√≥n Inmediata

**Plan de Acci√≥n (Orden de Ejecuci√≥n):**

### Fase 1: Optimizaciones Seguras (1-2 d√≠as)
1. ‚úÖ Reducir pre-re-ranking 50 ‚Üí 20
2. ‚úÖ Skip re-ranking si `metadata_filter` activo
3. ‚úÖ Revertir `top_k=50` ‚Üí `top_k=30`

**Test:** Evaluar 8 tests cr√≠ticos ‚Üí Objetivo mantener 6/8

### Fase 2: Router Inteligente (2-3 d√≠as)
4. Implementar clasificador SIMPLE vs COMPLEX
5. Usar `gpt-4o-mini` para queries SIMPLE

**Test:** Comparar calidad de respuestas en 20 queries variadas

### Fase 3: GGUF (Largo plazo, 1-2 semanas)
6. Experimentar con Llama 3.1 70B Q4_K_M para s√≠ntesis
7. Validar accuracy no baja > 5%

---

## üí° Conclusi√≥n

**Los n√∫meros altos (top_k=50, pre-reranking=50) NO son por capricho**, sino por experimentos previos que mostraron:
- top_k=10 ‚Üí 60% accuracy
- top_k=30 ‚Üí 76.7% accuracy
- top_k=50 ‚Üí 75% accuracy (REGRESI√ìN por ruido + latencia catastr√≥fica)

**Lecci√≥n aprendida:** M√°s datos != mejor accuracy. La clave es **calidad sobre cantidad** + **optimizaciones inteligentes** (metadata boosting, legislative boost).

**Pr√≥ximos pasos:** Implementar Tier 1 (bajo riesgo) y medir impacto antes de optimizaciones agresivas.
