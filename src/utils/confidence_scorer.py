"""
Sistema de scoring de confianza para respuestas del RAG.
Combina m√∫ltiples se√±ales para calcular confianza 0-100%.
"""

import re
import logging
from typing import List, Dict, Any

logger = logging.getLogger(__name__)

class ConfidenceScorer:
    """Calcula confianza de respuesta bas√°ndose en m√∫ltiples factores"""
    
    def __init__(self):
        # Pesos de cada factor (suman 100)
        self.weights = {
            "retrieval_quality": 30,    # Calidad del top chunk
            "consensus": 25,             # Acuerdo entre chunks
            "specificity": 20,           # Respuesta concreta vs gen√©rica
            "validation": 25             # Resultados del validator
        }
    
    def score_answer(
        self,
        answer: str,
        query: str,
        chunks_with_scores: List[tuple],  # [(chunk, score), ...]
        validation_result: Dict = None
    ) -> Dict[str, Any]:
        """
        Calcula score de confianza 0-100%
        
        Args:
            answer: Respuesta generada
            query: Query original del usuario
            chunks_with_scores: Lista de (chunk, score) del retrieval
            validation_result: Output del Answer Validator (opcional)
        
        Returns:
            {
                "confidence": float (0-100),
                "breakdown": Dict[str, int],
                "recommendation": str,
                "factors": Dict[str, Any]
            }
        """
        logger.info("üéØ Calculando confidence score...")
        
        # Factor 1: Calidad del Retrieval
        retrieval_score = self._score_retrieval_quality(chunks_with_scores)
        
        # Factor 2: Consenso entre chunks
        consensus_score = self._score_consensus(answer, chunks_with_scores)
        
        # Factor 3: Especificidad de la respuesta
        specificity_score = self._score_specificity(answer, query)
        
        # Factor 4: Validaci√≥n (si est√° disponible)
        if validation_result:
            validation_score = self._score_validation(validation_result)
        else:
            validation_score = 50  # Neutral si no hay validaci√≥n
        
        # Calcular score ponderado
        breakdown = {
            "retrieval_quality": retrieval_score,
            "consensus": consensus_score,
            "specificity": specificity_score,
            "validation": validation_score
        }
        
        confidence = sum(
            (breakdown[factor] / 100) * self.weights[factor]
            for factor in self.weights
        )
        
        # Generar recomendaci√≥n
        recommendation = self._get_recommendation(confidence, breakdown)
        
        # Log detallado
        logger.info(f"üìä Breakdown:")
        for factor, score in breakdown.items():
            logger.info(f"  - {factor}: {score}/100")
        logger.info(f"üéØ Confidence final: {confidence:.1f}%")
        logger.info(f"üí° Recomendaci√≥n: {recommendation}")
        
        return {
            "confidence": round(confidence, 1),
            "breakdown": breakdown,
            "recommendation": recommendation,
            "factors": {
                "top_chunk_score": chunks_with_scores[0][1] if chunks_with_scores else 0,
                "chunks_analyzed": len(chunks_with_scores),
                "answer_length": len(answer.split()),
                "has_numbers": bool(re.search(r'\d+', answer)),
                "has_citations": bool(re.search(r'\[(?:Doc|Fuente):', answer))
            }
        }
    
    # ========== SCORING DE FACTORES INDIVIDUALES ==========
    
    def _score_retrieval_quality(self, chunks_with_scores: List[tuple]) -> int:
        """
        Score basado en calidad del top chunk
        
        Score > 0.9 ‚Üí 100 puntos
        Score > 0.7 ‚Üí 70 puntos
        Score > 0.5 ‚Üí 40 puntos
        Score < 0.5 ‚Üí 20 puntos
        """
        if not chunks_with_scores:
            return 0
        
        # Validar si chunks_with_scores tiene elementos y si estos tienen score
        try:
             top_score = chunks_with_scores[0][1]
        except (IndexError, TypeError):
             return 0

        
        if top_score > 0.9:
            return 100
        elif top_score > 0.7:
            return 70
        elif top_score > 0.5:
            return 40
        else:
            return 20
    
    def _score_consensus(self, answer: str, chunks_with_scores: List[tuple]) -> int:
        """
        Score basado en consenso entre chunks
        
        Compara respuesta con m√∫ltiples chunks para ver si
        hay acuerdo en la informaci√≥n.
        """
        if len(chunks_with_scores) < 3:
            return 50  # Neutral si pocos chunks
        
        # Extraer entidades clave de la respuesta
        key_entities = self._extract_key_entities(answer)
        
        if not key_entities:
            return 50  # Neutral si no hay entidades espec√≠ficas
        
        # Contar cu√°ntos chunks contienen estas entidades
        top_chunks = chunks_with_scores[:5]  # Considerar top-5
        chunk_texts = [chunk[0].get("contenido", "") if isinstance(chunk[0], dict) else (chunk[0].page_content if hasattr(chunk[0], 'page_content') else str(chunk[0]))
                       for chunk in top_chunks]
        
        matches = 0
        for entity in key_entities:
            # Contar en cu√°ntos chunks aparece
            count = sum(1 for text in chunk_texts if entity.lower() in text.lower())
            if count >= 2:  # Al menos 2 chunks confirman
                matches += 1
        
        # Score proporcional al consenso
        consensus_rate = matches / len(key_entities) if key_entities else 0
        
        if consensus_rate >= 0.8:
            return 100  # Alto consenso
        elif consensus_rate >= 0.5:
            return 70   # Consenso moderado
        elif consensus_rate >= 0.3:
            return 40   # Consenso bajo
        else:
            return 20   # Sin consenso
    
    def _score_specificity(self, answer: str, query: str) -> int:
        """
        Score basado en especificidad de la respuesta
        
        Penaliza respuestas gen√©ricas tipo:
        - "No se encontr√≥ informaci√≥n"
        - "Seg√∫n los documentos..."
        - Respuestas muy cortas (<20 palabras)
        """
        # Patrones de respuestas gen√©ricas
        generic_patterns = [
            r"no\s+(?:se|est√°|consta|aparece|encuentra)",
            r"seg√∫n\s+(?:el|los|la|las)\s+documento",
            r"puede\s+(?:consultar|ver|revisar)",
            r"informaci√≥n\s+no\s+disponible",
        ]
        
        # Penalizar si es gen√©rica
        if any(re.search(p, answer.lower()) for p in generic_patterns):
            return 20
        
        # Penalizar respuestas muy cortas
        word_count = len(answer.split())
        if word_count < 20:
            return 40
        
        # Bonus por elementos espec√≠ficos
        score = 60  # Base
        
        # +10 si tiene n√∫meros
        if re.search(r'\d+', answer):
            score += 10
        
        # +10 si tiene fechas
        if re.search(r'\d{1,2}/\d{1,2}/\d{4}', answer):
            score += 10
        
        # +10 si tiene normativas (ISO, STANAG, etc)
        if re.search(r'(?:ISO|STANAG|MIL-STD|UNE-EN)\s*[\w\-:]+', answer):
            score += 10
        
        # +10 si tiene citaciones
        if re.search(r'\[(?:Doc|Fuente):', answer):
            score += 10
        
        return min(score, 100)
    
    def _score_validation(self, validation_result: Dict) -> int:
        """
        Score basado en resultados del Answer Validator
        
        100: Todas las capas v√°lidas
        70: 2/3 capas v√°lidas
        40: 1/3 capas v√°lidas
        0: 0/3 capas v√°lidas
        """
        valid_layers = sum([
            validation_result.get("numerical", {}).get("valid", False),
            validation_result.get("logical", {}).get("valid", False),
            validation_result.get("citation", {}).get("valid", False)
        ])
        
        if valid_layers == 3:
            return 100
        elif valid_layers == 2:
            return 70
        elif valid_layers == 1:
            return 40
        else:
            return 0
    
    # ========== M√âTODOS AUXILIARES ==========
    
    def _extract_key_entities(self, text: str) -> List[str]:
        """Extrae entidades clave (n√∫meros, fechas, normativas, nombres)"""
        entities = []
        
        # Importes
        importes = re.findall(r'\d{1,3}(?:[.,]\d{3})*(?:[.,]\d{2})?\s*(?:‚Ç¨|EUR)', text)
        entities.extend(importes)
        
        # Fechas
        fechas = re.findall(r'\d{1,2}/\d{1,2}/\d{4}', text)
        entities.extend(fechas)
        
        # Normativas
        normativas = re.findall(r'(?:ISO|STANAG|MIL-STD|UNE-EN)\s*[\w\-:]+', text)
        entities.extend(normativas)
        
        # CIFs
        cifs = re.findall(r'\b[A-Z]-?\d{8}\b', text)
        entities.extend(cifs)
        
        # Contratos (ej: CON_2024_012)
        contratos = re.findall(r'[A-Z]{3}_\d{4}_\d{3}', text)
        entities.extend(contratos)
        
        return list(set(entities))  # √önicos
    
    def _get_recommendation(self, confidence: float, breakdown: Dict) -> str:
        """Genera recomendaci√≥n basada en score"""
        
        if confidence >= 90:
            return "‚úÖ ALTA CONFIANZA - Respuesta validada y fiable"
        
        elif confidence >= 70:
            return "üü¢ CONFIANZA BUENA - Respuesta aceptable, revisar si es cr√≠tica"
        
        elif confidence >= 50:
            # Identificar factor d√©bil
            weak_factor = min(breakdown, key=breakdown.get)
            return f"üü° CONFIANZA MEDIA - Factor d√©bil: {weak_factor}. Revisar manualmente"
        
        else:
            # Identificar factores cr√≠ticos
            critical = [f for f, score in breakdown.items() if score < 40]
            return f"üî¥ BAJA CONFIANZA - Problemas en: {', '.join(critical)}. Requiere validaci√≥n humana"


# ========== FUNCI√ìN HELPER ==========

def calculate_confidence(
    answer: str,
    query: str,
    chunks_with_scores: List[tuple],
    validation_result: Dict = None
) -> Dict:
    """
    Wrapper simple para calcular confianza
    
    Usage:
        confidence = calculate_confidence(answer, query, chunks, validation)
        print(f"Confianza: {confidence['confidence']}%")
    """
    scorer = ConfidenceScorer()
    return scorer.score_answer(answer, query, chunks_with_scores, validation_result)
