import sys
import logging
from pathlib import Path

# A√±adir src al path
sys.path.append(str(Path(__file__).resolve().parent))

from src.agents.rag_agent import build_metadata_context, classify_query, format_conversation_history
from src.config import RAG_SYSTEM_PROMPT
from src.utils.llm_config import generate_response

# Configura logging para ver qu√© pasa
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def run_test():
    print("ü§ñ INICIANDO TEST DE DEBUG DEL CHATBOT...\n")
    
    # 1. Verificar Contexto de Metadata
    print("üìä 1. Generando contexto de metadata...")
    context = build_metadata_context()
    print("-" * 40)
    print(context)
    print("-" * 40)
    
    # Verificar si el contrato de 12.5M aparece y si est√° ordenado
    if "12.500.000,00‚Ç¨" in context:
        print("‚úÖ El contrato de 12.5M (CON_2024_009) est√° en el contexto")
    else:
        print("‚ùå ALERTA: El contrato de 12.5M NO aparece en el contexto")

    # 2. Probar preguntas clave
    questions = [
        "¬øCu√°l es el contrato de mayor importe?",
        "¬øQu√© contrato vence antes?",
        "¬øCu√°l es el contrato de suministros de mayor valor?"
    ]
    
    for i, question in enumerate(questions, 1):
        print(f"\n‚ùì PREGUNTA {i}: {question}")
        
        # Clasificaci√≥n
        q_type = classify_query(question)
        print(f"   Tipo detectado: {q_type}")
        
        # Generar Prompt
        historial = ""
        prompt = RAG_SYSTEM_PROMPT.format(
            contexto=context if q_type == 'QUANTITATIVE' else "CONTEXTO_DUMMY",
            historial=historial,
            pregunta=question
        )
        
        print("   üß† Generando respuesta con LLM...")
        response = generate_response(prompt, max_tokens=200, temperature=0.0)
        
        print(f"   ü§ñ RESPUESTA: {response}\n")

if __name__ == "__main__":
    run_test()
