# -*- coding: utf-8 -*-
"""
Final Evaluation Script (Day 7)
Runs the full Golden Dataset (20 questions) and Adversarial Tests against the Agentic RAG system.
Generates a comprehensive report in JSON and Markdown formats.
"""

import sys
import json
import time
from pathlib import Path
from typing import List, Dict, Any

# Add src to path
sys.path.insert(0, str(Path(__file__).resolve().parent))

from src.graph.workflow import run_agentic_rag

# --- GOLDEN DATASET (20 QUESTIONS) ---
GOLDEN_DATASET = [
    {"id": "ADV001", "type": "Aggregation", "query": "¬øCu√°l es el importe total acumulado de todos los avales bancarios en el sistema?", "keywords": ["avales", "total"]},
    {"id": "ADV002", "type": "Ranking", "query": "Lista los 4 contratos con mayor importe total, ordenados de mayor a menor", "keywords": ["CON_2024_012", "SER_2024_015", "CON_2024_018"]},
    {"id": "ADV003", "type": "Filtrado Complejo", "query": "¬øQu√© contratos tienen nivel de seguridad SECRETO Y un importe superior a 5 millones de euros?", "keywords": ["CON_2024_012"]},
    {"id": "ADV004", "type": "Agregaci√≥n Condicional", "query": "¬øCu√°ntos contratos vencen entre febrero y abril de 2026?", "keywords": ["vencen", "2026"]},
    {"id": "ADV005", "type": "Comparaci√≥n", "query": "Compara los plazos de ejecuci√≥n entre el contrato de mayor y menor importe total", "keywords": ["plazo", "d√≠as"]},
    {"id": "ADV006", "type": "An√°lisis Cruzado", "query": "¬øQu√© entidad bancaria avala el mayor importe acumulado y cu√°nto es?", "keywords": ["banco", "avala"]},
    {"id": "ADV007", "type": "Temporal + Num√©rico", "query": "¬øCu√°ntos d√≠as faltan para que venzan los avales cuyo importe supera los 400.000‚Ç¨?", "keywords": ["d√≠as", "venzan"]},
    {"id": "ADV008", "type": "Razonamiento", "query": "¬øQu√© contratos de OBRAS tienen penalizaciones por retraso superiores a 5.000‚Ç¨/d√≠a?", "keywords": ["OBRAS", "penalizaci√≥n"]},
    {"id": "ADV009", "type": "Identificaci√≥n de Riesgos", "query": "De los contratos que vencen en los pr√≥ximos 30 d√≠as, ¬øcu√°les son de tipo SERVICIOS?", "keywords": ["SERVICIOS"]},
    {"id": "ADV010", "type": "Agregaci√≥n Porcentual", "query": "¬øQu√© porcentaje del importe total de todos los contratos representa el contrato de mayor valor?", "keywords": ["porcentaje"]},
    {"id": "ADV011", "type": "Multi-documento", "query": "¬øCu√°ntos contratos diferentes mencionan normativas STANAG?", "keywords": ["STANAG"]},
    {"id": "ADV012", "type": "Comparaci√≥n Temporal", "query": "¬øQu√© contrato tiene el plazo de ejecuci√≥n m√°s largo y cu√°ntos d√≠as dura?", "keywords": ["plazo", "largo"]},
    {"id": "ADV013", "type": "Multicriterio", "query": "Lista los contratos con avales de Banco Santander cuyo importe de aval sea superior a 200.000‚Ç¨", "keywords": ["Santander", "200.000"]},
    {"id": "ADV014", "type": "An√°lisis de Frecuencias", "query": "¬øCu√°l es el c√≥digo STANAG que aparece en m√°s contratos distintos?", "keywords": ["STANAG"]},
    {"id": "ADV015", "type": "Umbral + Ranking", "query": "De los contratos con importe superior a 10 millones, ¬øcu√°l tiene la penalizaci√≥n por d√≠a m√°s alta?", "keywords": ["penalizaci√≥n", "alta"]},
    {"id": "ADV016", "type": "Temporal Complejo", "query": "¬øQu√© contratos se firmaron en el Q3 de 2024 (julio-septiembre)?", "keywords": ["2024", "firmaron"]},
    {"id": "ADV017", "type": "Cruce de Datos", "query": "¬øQu√© contratos clasificados como CONFIDENCIAL o SECRETO tienen subcontrataci√≥n prohibida?", "keywords": ["subcontrataci√≥n", "prohibida"]},
    {"id": "ADV018", "type": "Estad√≠stica", "query": "¬øCu√°l es el importe promedio de los contratos de tipo SUMINISTRO?", "keywords": ["promedio", "SUMINISTRO"]},
    {"id": "ADV019", "type": "Identificaci√≥n Cr√≠tica", "query": "¬øQu√© contrato tiene el aval que vence m√°s pronto y cu√°ntos d√≠as faltan?", "keywords": ["aval", "vence"]},
    {"id": "ADV020", "type": "Multi-agregaci√≥n", "query": "¬øCu√°ntos contratos hay por cada nivel de clasificaci√≥n de seguridad?", "keywords": ["SECRETO", "CONFIDENCIAL"]}
]

# --- ADVERSARIAL TEST ---
ADVERSARIAL_QUESTIONS = [
    {"id": "ADV_NEG_01", "type": "Negation", "query": "¬øQu√© contratos NO tienen cl√°usula de confidencialidad?", "keywords": ["no consta", "todos tienen"]},
    {"id": "ADV_HAL_01", "type": "Hallucination Check", "query": "¬øQu√© proveedores han fallado en las entregas?", "keywords": ["no consta", "informaci√≥n no disponible"]}
]

RESULTS_FILE = "final_evaluation_results.json"
REPORT_FILE = "final_evaluation_report.md"

def run_tests():
    print("="*80)
    print("üöÄ STARTING FINAL EVALUATION (DAY 7)")
    print(f"Dataset Size: {len(GOLDEN_DATASET)} Questions")
    print(f"Adversarial Tests: {len(ADVERSARIAL_QUESTIONS)} Questions")
    print("="*80)

    all_results = []
    
    # 1. Run Golden Dataset
    print("\n--- PHASE 1: GOLDEN DATASET ---\n")
    for i, item in enumerate(GOLDEN_DATASET):
        print(f"[{i+1}/{len(GOLDEN_DATASET)}] Running {item['id']} ({item['type']})...")
        start_time = time.time()
        try:
            result = run_agentic_rag(item['query'])
            latency = time.time() - start_time
            
            # Extract key metrics
            answer_text = result.get('answer', '')
            sources = result.get('sources', [])
            meta = result.get('metadata', {})
            
            outcome = {
                "id": item['id'],
                "type": item['type'],
                "query": item['query'],
                "answer": answer_text,
                "sources_count": len(sources),
                "latency": round(latency, 2),
                "complexity": meta.get('complexity', 'unknown'),
                "retry_count": meta.get('retry_count', 0),
                "eval_score": meta.get('evaluation_score', 0.0),
                "keywords_found": [k for k in item['keywords'] if k.lower() in answer_text.lower()]
            }
            all_results.append(outcome)
            print(f"   ‚úÖ Done in {outcome['latency']}s | Score: {outcome['eval_score']} | Sources: {outcome['sources_count']}")
            print("-" * 40)
            
        except Exception as e:
            print(f"   ‚ùå ERROR: {e}")
            all_results.append({
                "id": item['id'],
                "error": str(e)
            })

    # 2. Run Adversarial Tests
    print("\n--- PHASE 2: ADVERSARIAL TESTING ---\n")
    for item in ADVERSARIAL_QUESTIONS:
        print(f"Running {item['id']} ({item['type']})...")
        start_time = time.time()
        try:
            result = run_agentic_rag(item['query'])
            latency = time.time() - start_time
            
            outcome = {
                "id": item['id'],
                "type": item['type'],
                "query": item['query'],
                "answer": result.get('answer', ''),
                "latency": round(latency, 2),
                "retry_count": result.get('metadata', {}).get('retry_count', 0)
            }
            all_results.append(outcome)
            print(f"   ‚úÖ Done in {outcome['latency']}s")
            
        except Exception as e:
             print(f"   ‚ùå ERROR: {e}")
             all_results.append({"id": item['id'], "error": str(str)})

    # 3. Save Results
    with open(RESULTS_FILE, "w", encoding="utf-8") as f:
        json.dump(all_results, f, indent=4, ensure_ascii=False)
    
    print(f"\n‚úÖ Results saved to {RESULTS_FILE}")
    generate_markdown_report(all_results)

def generate_markdown_report(results):
    report = "# Final Evaluation Report (Day 7)\n\n"
    report += "| ID | Type | Latency (s) | Sources | Score | Keywords Found |\n"
    report += "|---|---|---|---|---|---|\n"
    
    for r in results:
        if "error" in r:
            report += f"| {r['id']} | ERROR | - | - | - | - |\n"
        else:
            kws = ", ".join(r.get('keywords_found', []))
            if r['id'].startswith("ADV_"): # Adversarial
                 report += f"| {r['id']} | {r['type']} | {r['latency']} | N/A | N/A | N/A |\n"
            else:
                 report += f"| {r['id']} | {r['type']} | {r['latency']} | {r['sources_count']} | {r['eval_score']} | {kws} |\n"

    with open(REPORT_FILE, "w", encoding="utf-8") as f:
        f.write(report)
    print(f"‚úÖ Report saved to {REPORT_FILE}")

if __name__ == "__main__":
    run_tests()
