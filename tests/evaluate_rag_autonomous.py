# -*- coding: utf-8 -*-
"""
EvaluaciÃ³n AutÃ³noma del Sistema RAG V3
Ejecuta evaluaciÃ³n end-to-end sin necesidad de servidor web
"""
import sys
import json
import logging
from pathlib import Path
from datetime import datetime

# Configurar path para imports
sys.path.insert(0, str(Path(__file__).resolve().parent.parent))

from src.agents.rag_agent import chat

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def evaluate_rag(dataset_path: Path):
    """EvalÃºa el RAG contra el golden dataset"""
    
    print("\n" + "="*80)
    print("ğŸ§ª EVALUACIÃ“N AUTÃ“NOMA DEL SISTEMA RAG V3")
    print("="*80 + "\n")
    
    # Cargar golden dataset
    with open(dataset_path, 'r', encoding='utf-8') as f:
        dataset = json.load(f)
    
    print(f"ğŸ“‹ Dataset cargado: {len(dataset)} preguntas\n")
    
    # Evaluar cada pregunta
    results = []
    correct = 0
    
    for item in dataset:
        q_id = item['id']
        pregunta = item['pregunta']
        respuesta_correcta = item['respuesta_correcta']
        
        print(f"[Q{q_id}] {pregunta}")
        
        try:
            # Obtener respuesta del RAG usando funciÃ³n chat
            respuesta = chat(pregunta)
            
            # Mostrar respuesta
            print(f"âœ… Respuesta obtenida: {respuesta[:200]}..." if len(respuesta) > 200 else f"âœ… Respuesta: {respuesta}")
            
            # EvaluaciÃ³n simple: verificar si la respuesta correcta estÃ¡ en la respuesta del RAG
            # (Esto es una aproximaciÃ³n; idealmente usarÃ­amos LLM para evaluar)
            is_correct = any(keyword.lower() in respuesta.lower() for keyword in respuesta_correcta.split() if len(keyword) > 3)
            
            if is_correct:
                correct += 1
                print("âœ… CORRECTO")
            else:
                print(f"âŒ INCORRECTO - Esperado: {respuesta_correcta}")
            
            results.append({
                "id": q_id,
                "pregunta": pregunta,
                "respuesta_rag": respuesta,
                "respuesta_correcta": respuesta_correcta,
                "correcto": is_correct
            })
            
        except Exception as e:
            print(f"âŒ ERROR: {e}")
            results.append({
                "id": q_id,
                "pregunta": pregunta,
                "respuesta_rag": f"ERROR: {str(e)}",
                "respuesta_correcta": respuesta_correcta,
                "correcto": False
            })
        
        print()
    
    # Calcular accuracy
    accuracy = (correct / len(dataset)) * 100
    
    # Guardar resultados
    results_path = Path(__file__).parent / "evaluation_results_v3.json"
    with open(results_path, 'w', encoding='utf-8') as f:
        json.dump({
            "timestamp": datetime.now().isoformat(),
            "total_questions": len(dataset),
            "correct_answers": correct,
            "accuracy_percentage": accuracy,
            "details": results
        }, f, indent=2, ensure_ascii=False)
    
    # Resumen final
    print("\n" + "="*80)
    print("ğŸ“Š RESULTADOS DE EVALUACIÃ“N")
    print("="*80)
    print(f"Total Preguntas:    {len(dataset)}")
    print(f"Respuestas Correctas: {correct}")
    print(f"Accuracy:           {accuracy:.1f}%")
    print(f"\nğŸ“„ Resultados guardados en: {results_path}")
    print("="*80 + "\n")
    
    return accuracy, results

def main():
    dataset_path = Path(__file__).parent / "golden_dataset_v3.json"
    
    if not dataset_path.exists():
        print(f"âŒ ERROR: No se encontrÃ³ {dataset_path}")
        return
    
    accuracy, results = evaluate_rag(dataset_path)
    
    # Reporte comparativo
    print("\n" + "="*80)
    print("ğŸ“ˆ COMPARATIVA CON VERSIÃ“N ANTERIOR")
    print("="*80)
    print(f"Accuracy Anterior:  46.7%")
    print(f"Accuracy Actual:    {accuracy:.1f}%")
    print(f"Mejora:             {accuracy - 46.7:+.1f} puntos porcentuales")
    print("="*80 + "\n")

if __name__ == "__main__":
    main()
